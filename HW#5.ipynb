{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file = open('translation2019zh_valid1.json' , 'r',encoding='utf-8') \n",
    "\n",
    "en_data=[]\n",
    "ch_data=[]\n",
    "\n",
    "for line in file.readlines():\n",
    "    tmp=json.loads(line)\n",
    "    en_data.append(tmp['english'])\n",
    "    ch_data.append(tmp['chinese'])\n",
    "\n",
    "#print(en[5161433])\n",
    "#print(ch[5161433])\n",
    "#print(en[:10])\n",
    "#print(ch[:10])\n",
    "\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分別生成中英文字典\n",
    "en_vocab = set(''.join(en_data))\n",
    "id2en = list(en_vocab)\n",
    "en2id = {c:i for i,c in enumerate(id2en)}\n",
    "\n",
    "# 分別生成中英文字典\n",
    "ch_vocab = set(''.join(ch_data))\n",
    "id2ch = list(ch_vocab)\n",
    "ch2id = {c:i for i,c in enumerate(id2ch)}\n",
    "\n",
    "#print(en2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "char: I didn't own a Thesaurus until four years ago and I use a small Webster's dictionary that I'd bought at K-Mart for 89 cents.\n",
      "index: [98, 38, 154, 25, 154, 108, 104, 152, 38, 110, 59, 108, 38, 2, 38, 36, 48, 57, 39, 2, 139, 144, 139, 39, 38, 139, 108, 152, 25, 76, 38, 37, 110, 139, 144, 38, 5, 57, 2, 144, 39, 38, 2, 24, 110, 38, 2, 108, 154, 38, 98, 38, 139, 39, 57, 38, 2, 38, 39, 43, 2, 76, 76, 38, 83, 57, 75, 39, 152, 57, 144, 104, 39, 38, 154, 25, 14, 152, 25, 110, 108, 2, 144, 5, 38, 152, 48, 2, 152, 38, 98, 104, 154, 38, 75, 110, 139, 24, 48, 152, 38, 2, 152, 38, 40, 116, 10, 2, 144, 152, 38, 37, 110, 144, 38, 85, 7, 38, 14, 57, 108, 152, 39, 78]\n"
     ]
    }
   ],
   "source": [
    "en_num_data = [[en2id[en] for en in line ] for line in en_data]\n",
    "ch_num_data = [[ch2id[ch] for ch in line] for line in ch_data]\n",
    "de_num_data = [[ch2id[ch] for ch in line][1:] for line in ch_data]\n",
    "\n",
    "print('char:', en_data[1])\n",
    "print('index:', en_num_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max encoder length: 256\n",
      "max decoder length: 174\n",
      "index data:\n",
      " [98, 38, 154, 25, 154, 108, 104, 152, 38, 110, 59, 108, 38, 2, 38, 36, 48, 57, 39, 2, 139, 144, 139, 39, 38, 139, 108, 152, 25, 76, 38, 37, 110, 139, 144, 38, 5, 57, 2, 144, 39, 38, 2, 24, 110, 38, 2, 108, 154, 38, 98, 38, 139, 39, 57, 38, 2, 38, 39, 43, 2, 76, 76, 38, 83, 57, 75, 39, 152, 57, 144, 104, 39, 38, 154, 25, 14, 152, 25, 110, 108, 2, 144, 5, 38, 152, 48, 2, 152, 38, 98, 104, 154, 38, 75, 110, 139, 24, 48, 152, 38, 2, 152, 38, 40, 116, 10, 2, 144, 152, 38, 37, 110, 144, 38, 85, 7, 38, 14, 57, 108, 152, 39, 78]\n",
      "one hot data:\n",
      " [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 獲取輸入輸出端的最大長度\n",
    "max_encoder_seq_length = max([len(txt) for txt in en_num_data])\n",
    "max_decoder_seq_length = max([len(txt) for txt in ch_num_data])\n",
    "print('max encoder length:', max_encoder_seq_length)\n",
    "print('max decoder length:', max_decoder_seq_length)\n",
    "\n",
    "# 將數據進行onehot處理\n",
    "encoder_input_data = np.zeros((len(en_num_data), max_encoder_seq_length, len(en2id)), dtype='float16')\n",
    "decoder_input_data = np.zeros((len(ch_num_data), max_decoder_seq_length, len(ch2id)), dtype='float16')\n",
    "decoder_target_data = np.zeros((len(ch_num_data), max_decoder_seq_length, len(ch2id)), dtype='float16')\n",
    "\n",
    "for i in range(len(ch_num_data)):\n",
    "    for t, j in enumerate(en_num_data[i]):\n",
    "        encoder_input_data[i, t, j] = 1.\n",
    "    for t, j in enumerate(ch_num_data[i]):\n",
    "        decoder_input_data[i, t, j] = 1.\n",
    "    for t, j in enumerate(de_num_data[i]):\n",
    "        decoder_target_data[i, t, j] = 1.\n",
    "\n",
    "print('index data:\\n', en_num_data[1])\n",
    "print('one hot data:\\n', encoder_input_data[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "EN_VOCAB_SIZE = len(en2id)\n",
    "CH_VOCAB_SIZE = len(ch2id)\n",
    "HIDDEN_SIZE = 256\n",
    "\n",
    "LEARNING_RATE = 0.003\n",
    "BATCH_SIZE = 200\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Embedding\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "# ==============encoder=============\n",
    "encoder_inputs = Input(shape=(None, EN_VOCAB_SIZE))\n",
    "#emb_inp = Embedding(output_dim=HIDDEN_SIZE, input_dim=EN_VOCAB_SIZE)(encoder_inputs)\n",
    "encoder_h1, encoder_state_h1, encoder_state_c1 = LSTM(HIDDEN_SIZE, return_sequences=True, return_state=True)(encoder_inputs)\n",
    "encoder_h2, encoder_state_h2, encoder_state_c2 = LSTM(HIDDEN_SIZE, return_state=True)(encoder_h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = Input(shape=(None, CH_VOCAB_SIZE))\n",
    "\n",
    "#emb_target = Embedding(output_dim=HIDDEN_SIZE, input_dim=CH_VOCAB_SIZE, mask_zero=True)(decoder_inputs)\n",
    "lstm1 = LSTM(HIDDEN_SIZE, return_sequences=True, return_state=True)\n",
    "lstm2 = LSTM(HIDDEN_SIZE, return_sequences=True, return_state=True)\n",
    "decoder_dense = Dense(CH_VOCAB_SIZE, activation='softmax')\n",
    "\n",
    "decoder_h1, _, _ = lstm1(decoder_inputs, initial_state=[encoder_state_h1, encoder_state_c1])\n",
    "decoder_h2, _, _ = lstm2(decoder_h1, initial_state=[encoder_state_h2, encoder_state_c2])\n",
    "decoder_outputs = decoder_dense(decoder_h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, None, 159)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            (None, None, 2919)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                   [(None, None, 256),  425984      input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                   [(None, None, 256),  3252224     input_3[0][0]                    \n",
      "                                                                 lstm_3[0][1]                     \n",
      "                                                                 lstm_3[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_4 (LSTM)                   [(None, 256), (None, 525312      lstm_3[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "lstm_6 (LSTM)                   [(None, None, 256),  525312      lstm_5[0][0]                     \n",
      "                                                                 lstm_4[0][1]                     \n",
      "                                                                 lstm_4[0][2]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 2919)   750183      lstm_6[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 5,479,015\n",
      "Trainable params: 5,479,015\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\math_grad.py:102: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "Epoch 1/50\n",
      "2000/2000 [==============================] - 290s 145ms/step - loss: 1.4526 - accuracy: 0.0360\n",
      "Epoch 2/50\n",
      "2000/2000 [==============================] - 308s 154ms/step - loss: 1.3439 - accuracy: 0.0067\n",
      "Epoch 3/50\n",
      "2000/2000 [==============================] - 250s 125ms/step - loss: 1.3319 - accuracy: 0.0080\n",
      "Epoch 4/50\n",
      "2000/2000 [==============================] - 241s 121ms/step - loss: 1.3250 - accuracy: 0.0080\n",
      "Epoch 5/50\n",
      "2000/2000 [==============================] - 240s 120ms/step - loss: 1.3210 - accuracy: 0.0087\n",
      "Epoch 6/50\n",
      "2000/2000 [==============================] - 243s 122ms/step - loss: 1.3185 - accuracy: 0.0089\n",
      "Epoch 7/50\n",
      "2000/2000 [==============================] - 248s 124ms/step - loss: 1.3167 - accuracy: 0.0091\n",
      "Epoch 8/50\n",
      "2000/2000 [==============================] - 245s 123ms/step - loss: 1.3237 - accuracy: 0.0089\n",
      "Epoch 9/50\n",
      "2000/2000 [==============================] - 247s 123ms/step - loss: 1.3192 - accuracy: 0.0092\n",
      "Epoch 10/50\n",
      "2000/2000 [==============================] - 249s 125ms/step - loss: 1.3116 - accuracy: 0.0093\n",
      "Epoch 11/50\n",
      "2000/2000 [==============================] - 263s 132ms/step - loss: 1.3099 - accuracy: 0.0094\n",
      "Epoch 12/50\n",
      "2000/2000 [==============================] - 318s 159ms/step - loss: 1.3118 - accuracy: 0.0093\n",
      "Epoch 13/50\n",
      "2000/2000 [==============================] - 326s 163ms/step - loss: 1.3119 - accuracy: 0.0090\n",
      "Epoch 14/50\n",
      "2000/2000 [==============================] - 323s 162ms/step - loss: 1.3069 - accuracy: 0.0090\n",
      "Epoch 15/50\n",
      "2000/2000 [==============================] - 322s 161ms/step - loss: 1.3007 - accuracy: 0.0091\n",
      "Epoch 16/50\n",
      "2000/2000 [==============================] - 321s 161ms/step - loss: 1.2953 - accuracy: 0.0091\n",
      "Epoch 17/50\n",
      "2000/2000 [==============================] - 322s 161ms/step - loss: 1.2895 - accuracy: 0.0091\n",
      "Epoch 18/50\n",
      "2000/2000 [==============================] - 327s 164ms/step - loss: 1.2867 - accuracy: 0.0093\n",
      "Epoch 19/50\n",
      "2000/2000 [==============================] - 326s 163ms/step - loss: 1.2808 - accuracy: 0.0093\n",
      "Epoch 20/50\n",
      "2000/2000 [==============================] - 332s 166ms/step - loss: 1.2764 - accuracy: 0.0093\n",
      "Epoch 21/50\n",
      "2000/2000 [==============================] - 326s 163ms/step - loss: 1.2711 - accuracy: 0.0094\n",
      "Epoch 22/50\n",
      "2000/2000 [==============================] - 326s 163ms/step - loss: 1.2635 - accuracy: 0.0096\n",
      "Epoch 23/50\n",
      "2000/2000 [==============================] - 325s 163ms/step - loss: 1.2572 - accuracy: 0.0100\n",
      "Epoch 24/50\n",
      "2000/2000 [==============================] - 331s 165ms/step - loss: 1.2432 - accuracy: 0.0103\n",
      "Epoch 25/50\n",
      "2000/2000 [==============================] - 326s 163ms/step - loss: 1.2316 - accuracy: 0.0106\n",
      "Epoch 26/50\n",
      "2000/2000 [==============================] - 343s 171ms/step - loss: 1.2253 - accuracy: 0.0111\n",
      "Epoch 27/50\n",
      "2000/2000 [==============================] - 337s 168ms/step - loss: 1.2160 - accuracy: 0.0117\n",
      "Epoch 28/50\n",
      "2000/2000 [==============================] - 338s 169ms/step - loss: 1.2089 - accuracy: 0.0121\n",
      "Epoch 29/50\n",
      "2000/2000 [==============================] - 332s 166ms/step - loss: 1.2021 - accuracy: 0.0125\n",
      "Epoch 30/50\n",
      "2000/2000 [==============================] - 332s 166ms/step - loss: 1.1938 - accuracy: 0.0130\n",
      "Epoch 31/50\n",
      "2000/2000 [==============================] - 337s 169ms/step - loss: 1.1878 - accuracy: 0.0135\n",
      "Epoch 32/50\n",
      "2000/2000 [==============================] - 339s 169ms/step - loss: 1.1753 - accuracy: 0.0142\n",
      "Epoch 33/50\n",
      "2000/2000 [==============================] - 331s 165ms/step - loss: 1.1659 - accuracy: 0.0147\n",
      "Epoch 34/50\n",
      "2000/2000 [==============================] - 338s 169ms/step - loss: 1.1562 - accuracy: 0.0150\n",
      "Epoch 35/50\n",
      "2000/2000 [==============================] - 332s 166ms/step - loss: 1.1459 - accuracy: 0.0159\n",
      "Epoch 36/50\n",
      "2000/2000 [==============================] - 331s 166ms/step - loss: 1.1363 - accuracy: 0.0170\n",
      "Epoch 37/50\n",
      "2000/2000 [==============================] - 335s 168ms/step - loss: 1.1276 - accuracy: 0.0174\n",
      "Epoch 38/50\n",
      "2000/2000 [==============================] - 360s 180ms/step - loss: 1.1175 - accuracy: 0.0182\n",
      "Epoch 39/50\n",
      "2000/2000 [==============================] - 338s 169ms/step - loss: 1.1066 - accuracy: 0.0186\n",
      "Epoch 40/50\n",
      "2000/2000 [==============================] - 334s 167ms/step - loss: 1.0967 - accuracy: 0.0195\n",
      "Epoch 41/50\n",
      "2000/2000 [==============================] - 348s 174ms/step - loss: 1.0874 - accuracy: 0.0201\n",
      "Epoch 42/50\n",
      "2000/2000 [==============================] - 342s 171ms/step - loss: 1.0809 - accuracy: 0.0205\n",
      "Epoch 43/50\n",
      "2000/2000 [==============================] - 337s 169ms/step - loss: 1.0674 - accuracy: 0.0218\n",
      "Epoch 44/50\n",
      "2000/2000 [==============================] - 339s 170ms/step - loss: 1.0555 - accuracy: 0.0225\n",
      "Epoch 45/50\n",
      "2000/2000 [==============================] - 343s 171ms/step - loss: 1.0445 - accuracy: 0.0232\n",
      "Epoch 46/50\n",
      "2000/2000 [==============================] - 343s 172ms/step - loss: 1.0354 - accuracy: 0.0241\n",
      "Epoch 47/50\n",
      "2000/2000 [==============================] - 339s 170ms/step - loss: 1.0256 - accuracy: 0.0248\n",
      "Epoch 48/50\n",
      "2000/2000 [==============================] - 341s 170ms/step - loss: 1.0171 - accuracy: 0.0254\n",
      "Epoch 49/50\n",
      "2000/2000 [==============================] - 358s 179ms/step - loss: 1.0092 - accuracy: 0.0263\n",
      "Epoch 50/50\n"
     ]
    }
   ],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "opt = Adam(lr=LEARNING_RATE, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          epochs=EPOCHS,\n",
    "          validation_split=0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, [encoder_state_h1, encoder_state_c1, encoder_state_h2, encoder_state_c2])\n",
    "\n",
    "# 預測模型中的decoder的初始化狀態需要傳入新的狀態\n",
    "decoder_state_input_h1 = Input(shape=(HIDDEN_SIZE,))\n",
    "decoder_state_input_c1 = Input(shape=(HIDDEN_SIZE,))\n",
    "decoder_state_input_h2 = Input(shape=(HIDDEN_SIZE,))\n",
    "decoder_state_input_c2 = Input(shape=(HIDDEN_SIZE,))\n",
    "\n",
    "# 使用傳入的值來初始化當前模型的輸入狀態\n",
    "decoder_h1, state_h1, state_c1 = lstm1(decoder_inputs, initial_state=[decoder_state_input_h1, decoder_state_input_c1])\n",
    "decoder_h2, state_h2, state_c2 = lstm2(decoder_h1, initial_state=[decoder_state_input_h2, decoder_state_input_c2])\n",
    "decoder_outputs = decoder_dense(decoder_h2)\n",
    "\n",
    "decoder_model = Model([decoder_inputs, decoder_state_input_h1, decoder_state_input_c1, decoder_state_input_h2, decoder_state_input_c2], \n",
    "                      [decoder_outputs, state_h1, state_c1, state_h2, state_c2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
